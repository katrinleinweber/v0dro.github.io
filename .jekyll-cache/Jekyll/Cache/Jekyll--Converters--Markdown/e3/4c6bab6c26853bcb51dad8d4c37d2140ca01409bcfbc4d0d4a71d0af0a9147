I"5<p>For my GSOC project, I’m trying to build an extension to NMatrix which will interface with a high performance C library for fast linear algebra calculations. Since one of the major problems affecting the usability and portability of NMatrix is the effort taken for installation (adding/removing dependencies etc.), it is imperative to ship the source of this high performance C library alongwith the ruby gem.</p>

<p>This leaves us with quite a few choices about the library that can be used. The most common and obvious interfaces for performing fast linear algebra calculations are LAPACK and BLAS. Thus the library bundled with the nmatrix extension must expose an interface similar to LAPACK and BLAS. Since ruby running on MRI can only interface with libraries having a C interface, the contenders in this regard are CLAPACK or LAPACKE for a LAPACK in C, and openBLAS or ATLAS for a BLAS interface.</p>

<p>I need to choose an appropriate BLAS and LAPACK interface based on its speed and usability, and to do so, I decided to build some quick ruby interfaces to these libraries and benchmark the <a href="https://software.intel.com/en-us/node/520973"><code>?gesv</code> function</a>  (used for solving <em>n</em> linear equations in <em>n</em> unknowns) present in all LAPACK interfaces, so as to get an idea of what would be the fastest. This would also test the speed of the BLAS implemetation since LAPACK primarily depends on BLAS for actual computations.</p>

<p>To create these benchmarks, <a href="https://github.com/v0dro/scratch/tree/master/ruby_c_exp">I made a couple of simple ruby gems</a> which linked against the binaries of these libraries. All these gems <a href="https://github.com/v0dro/scratch/blob/master/ruby_c_exp/nm_lapacke/lib/nm_lapacke.rb">define a module</a> which contains a method <code>solve_gesv</code>, which calls the C extension that interfaces with the C library. Each library was made in its own little ruby gem so as to nullify any unknown side effects and also to provide more clarity.</p>

<p>To test these libraries against each other, I used the following test code:</p>

<div class="language-ruby highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
<strong><a href="#n10" name="n10">10</a></strong>
<a href="#n11" name="n11">11</a>
<a href="#n12" name="n12">12</a>
<a href="#n13" name="n13">13</a>
<a href="#n14" name="n14">14</a>
<a href="#n15" name="n15">15</a>
<a href="#n16" name="n16">16</a>
<a href="#n17" name="n17">17</a>
<a href="#n18" name="n18">18</a>
<a href="#n19" name="n19">19</a>
</pre></td>
  <td class="code"><pre>
require <span style="background-color:hsla(0,100%,50%,0.05)"><span style="color:#710">'</span><span style="color:#D20">benchmark</span><span style="color:#710">'</span></span>

<span style="color:#036;font-weight:bold">Benchmark</span>.bm <span style="color:#080;font-weight:bold">do</span> |x|
  x.report <span style="color:#080;font-weight:bold">do</span>
    <span style="color:#00D">10000</span>.times <span style="color:#080;font-weight:bold">do</span>
      a = <span style="color:#036;font-weight:bold">NMatrix</span>.new([<span style="color:#00D">3</span>,<span style="color:#00D">3</span>], [<span style="color:#00D">76</span>, <span style="color:#00D">25</span>, <span style="color:#00D">11</span>,
                              <span style="color:#00D">27</span>, <span style="color:#00D">89</span>, <span style="color:#00D">51</span>,
                              <span style="color:#00D">18</span>, <span style="color:#00D">60</span>, <span style="color:#00D">32</span>], <span style="color:#606">dtype</span>: <span style="color:#A60">:float64</span>)
      b = <span style="color:#036;font-weight:bold">NMatrix</span>.new([<span style="color:#00D">3</span>,<span style="color:#00D">1</span>], [<span style="color:#00D">10</span>,
                               <span style="color:#00D">7</span>,
                              <span style="color:#00D">43</span>], <span style="color:#606">dtype</span>: <span style="color:#A60">:float64</span>)
      <span style="color:#036;font-weight:bold">NMatrix</span>::<span style="color:#036;font-weight:bold">CLAPACK</span>.solve_gesv(a,b)
      <span style="color:#777"># The `NMatrix::CLAPACK` is replaced with NMatrix::LAPACKE </span>
      <span style="color:#777"># or NMatrix::LAPACKE_ATLAS as per the underlying binding. Read the</span>
      <span style="color:#777"># source code for more details.</span>
    <span style="color:#080;font-weight:bold">end</span>
  <span style="color:#080;font-weight:bold">end</span>
<span style="color:#080;font-weight:bold">end</span>
</pre></td>
</tr></table>
</div>

<p>Here I will list the libraries that I used, the functions I interfaced with, the pros and cons of using each of these libraries, and of course the reported benchmarks:</p>

<h3 id="clapack-lapack-with-openblas-blas">CLAPACK (LAPACK) with openBLAS (BLAS)</h3>

<p><a href="http://www.netlib.org/clapack/">CLAPACK</a> is an F2C’d version of the original LAPACK written in FORTRAN. The creators have made some changes by hand because f2c spews out unnecessary code at times, but otherwise its pretty much as fast as the original LAPACK.</p>

<p>To interface with a BLAS implementation, CLAPACK uses a blas wrapper (blaswrap) to generate wrappers to the relevant CBLAS functions exposed by any BLAS implementation. The blaswrap source files and F2C source files are provided with the CLAPACK library.</p>

<p>The BLAS implementation that we’ll be using is <a href="http://www.openblas.net/">openBLAS</a>, which is a very stable and tested BLAS exposing a C interface. It is extremely simple to use and install, and configures itself automatically according to the computer it is being installed upon. It claims to achieve <a href="http://en.wikipedia.org/wiki/GotoBLAS">performance comparable to intel MKL</a>, which is phenomenal.</p>

<p>To compile CLAPACK with openBLAS, do the following:</p>

<ul>
  <li><code>cd</code> to your openBLAS directory and run <code>make NO_LAPACK=1</code>. This will create an openBLAS binary with the object files only for BLAS and CBLAS. LAPACK will not be compiled even though the source is present. This will generate a <code>.a</code> file which has a name that is similar to the processor that your computer uses. Mine was <code>libopenblas_sandybridgep-r0.2.13.a</code>.</li>
  <li>Now rename the openBLAS binary file to <code>libopenblas.a</code> so its easier to type and you lessen your chances of mistakes, and copy to your CLAPACK directory.</li>
  <li><code>cd</code> to your CLAPACK directory and open the <code>make.inc</code> file in your editor. In it, you should find a <code>BLASDIR</code> variable that points to the BLAS files to link against. Change the value of this variable to <code>../../libopenblas.a</code>.</li>
  <li>Now run <code>make f2clib</code> to make F2C library. This is needed for interconversion between C and FORTRAN data types.</li>
  <li>Then run <code>make lapacklib</code> from the CLAPACK root directory to compile CLAPACK against your specified implementation of CBLAS (openBLAS in this case).</li>
  <li>At the end of this process, you should end up with the CLAPACK, F2C and openBLAS binaries in your directory.</li>
</ul>

<p>Since the automation of this compilation process would take time, I copied these binaries to the gem and <a href="">wrote the extconf.rb</a> such that they link with these libraries.</p>

<p>On testing this with a ruby wrapper, the benchmarking code listed above yielded the following results:</p>

<pre><code>
    user     system      total        real
    0.190000   0.000000   0.190000 (  0.186355)
</code></pre>

<h3 id="lapacke-lapack-compiled-with-openblas-blas">LAPACKE (LAPACK) compiled with openBLAS (BLAS)</h3>

<p><a href="http://www.netlib.org/lapack/lapacke.html">LAPACKE</a> is the ‘official’ C interface to the FORTRAN-written LAPACK. It consists of two levels; a high level C interface for use with C programs and a low level one that talks to the original FORTRAN LAPACK code. This is not just an f2c’d version of LAPACK, and hence the design of this library is such that it is easy to create a bridge between C and FORTRAN.</p>

<p>For example, C has arrays stored in row-major format while FORTRAN had them column-major. To perform any computation, a matrix needs to be transposed to column-major form first and then be re-transposed to row-major form so as to yield correct results. This needs to be done by the programmer when using CLAPACK, but LAPACKE’s higher level interface accepts arguments (<a href="http://www.netlib.org/lapack/lapacke.html#_array_arguments">LAPACKE_ROW_MAJOR or LAPACKE_COL_MAJOR</a>) which specify whether the matrices passed to it are in row major or column major format. Thus extra (often unoptimized code) on part of the programmer for performing the tranposes is avoided.</p>

<p>To build binaries of LAPACKE compiled with openBLAS, just <code>cd</code> to your openBLAS source code directory and run <code>make</code>. This will generate a <code>.a</code> file with the binaries for LAPACKE and CBLAS interface of openBLAS.</p>

<p>LAPACKE benchmarks turn out to be faster mainly due to the absence of <a href="https://github.com/v0dro/scratch/blob/master/ruby_c_exp/nm_clapack/lib/nm_clapack.rb#L7">manual transposing by high-level code written in Ruby</a>  (the <a href="https://github.com/SciRuby/nmatrix/blob/master/lib/nmatrix/nmatrix.rb#L535">NMatrix#transpose</a> function in this case). I think performing the tranposing using openBLAS functions should remedy this problem.</p>

<p>The benchmarks for LAPACKE are:</p>

<pre><code>
    user     system      total        real
    0.150000   0.000000   0.150000 (  0.147790)
</code></pre>

<p>As you can see these are quite faster than CLAPACK with openBLAS, listed above.</p>

<h3 id="clapacklapack-with-atlasblas">CLAPACK(LAPACK) with ATLAS(BLAS)</h3>

<p>This is the combination that is currently in use with nmatrix. It involves installing the <code>libatlas-base-dev</code> package from the Debian repositories. This pacakage will load all the relevant clapack, atlas, blas and cblas binaries into your computer.</p>

<p>The benchmarks turned out to be:</p>

<pre><code>
    user     system      total        real
    0.130000   0.000000   0.130000 (  0.130056)
</code></pre>

<p>This is fast. But a big limitation on using this approach is that the CLAPACK library exposed by the <code>libatlas-base-dev</code> is outdated and no longer maintained. To top it all, it does not have all the functions that a LAPACK library is supposed to have.</p>

<h3 id="lapackelapack-with-atlasblas">LAPACKE(LAPACK) with ATLAS(BLAS)</h3>

<p>For this test case I compiled <a href="http://www.netlib.org/lapack/lapacke">LAPACKE (downloaded from netlib)</a> with an ATLAS implementation from the Debian repositories. I then included the generated static libraries in the sample ruby gem and compiled the gem against those.</p>

<p>To do this on your machine:</p>
<ul>
  <li>Install the package <code>libatlas-base-dev</code> with your package manager. This will install the ATLAS and CBLAS shared objects onto your system.</li>
  <li><code>cd</code> to the lapack library and in the <code>make.inc</code> file change the <code>BLASLIB = -lblas -lcblas -latlas</code>. Then run <code>make</code>. This will compile LAPACK with ATLAS installed on your system.</li>
  <li>Then <code>cd</code> to the lacpack/lapacke folder and run <code>make</code>.</li>
</ul>

<p>Again the function chosen was <code>LAPACKE_?gesv</code>. This test should tell us a great deal about the speed differences between openBLAS and ATLAS, since tranposing overheads are handled by LAPACKE and no Ruby code is interfering with the benchmarks.</p>

<p>The benchmarks turned out to be:</p>

<pre><code>
    user     system      total        real
    0.140000   0.000000   0.140000 (  0.140540)
</code></pre>

<h2 id="conclusion">Conclusion</h2>

<p>As you can see from the benchmarks above, the approach followed by nmatrix currently (CLAPACK with ATLAS) is the fastest, but this approach has certain limitations:</p>

<ul>
  <li>Requires installation of tedious to install dependencies.</li>
  <li>Many pacakages offer the same binaries, causing confusion.</li>
  <li>CLAPACK library is outdated and not maintained any longer.</li>
  <li>ATLAS-CLAPACK does not expose all the functions present in LAPACK.</li>
</ul>

<p>The LAPACKE-openBLAS and the LAPACKE-ATLAS, though a little slower(~10-20 ms), offer a HUGE advantage over CLAPACK-ATLAS, viz. :</p>

<ul>
  <li>LAPACKE is the ‘standard’ C interface to the LAPACK libraries and is actively maintained, with regular release cycles.</li>
  <li>LAPACKE is compatible with intel’s MKL, in case a future need arises.</li>
  <li>LAPACKE bridges the differences between C and FORTRAN with a well thought out interface.</li>
  <li>LAPACKE exposes the entire LAPACK interface.</li>
  <li>openBLAS is trivial to install.</li>
  <li>ATLAS is a little non-trivial to install but is fast.</li>
</ul>

<p>For a further explanation of the differences between these CBLAS, CLAPACK and LAPACKE, read <a href="http://nicolas.limare.net/pro/notes/2014/10/31_cblas_clapack_lapacke/">this</a> blog post.</p>
:ET