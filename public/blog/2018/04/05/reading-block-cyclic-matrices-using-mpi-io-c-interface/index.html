<p><a href="https://stackoverflow.com/questions/10341860/mpi-io-reading-and-writing-block-cyclic-matrix#_=_">This</a> answer on stack overflow is pretty detailed for this purpose. Since the answer is in FORTRAN, I’ll explain with some C code and how I went about this.</p>

<p>A very cumbersome way of reading a row-major matrix from a file into an MPI process is to read individual chunks one by one in a block cyclic manner in a loop. A better way is to use the <a href="https://www.mpich.org/static/docs/v3.1/www3/MPI_Type_create_darray.html">MPI darray type</a> that is useful for reading chunks of the file directly without writing too much code. MPI lets you define a ‘view’ of a file and each process can just read its part of the view. It lets you define “distributed array” data types which you can use for directly reading a matrix stored in a file into memory in a block cyclic manner accoridng to the co-ordinates of the processor. We use the <code>MPI_Type_create_darray</code> <a href="http://mpi.deino.net/mpi_functions/MPI_Type_create_darray.html">function</a> for this purpose.</p>

<p>Here’s a sample usage of this function for initializing a <code>MPI_darray</code>:</p>
<div class="language-c highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
<a href="#n2" name="n2">2</a>
<a href="#n3" name="n3">3</a>
<a href="#n4" name="n4">4</a>
<a href="#n5" name="n5">5</a>
<a href="#n6" name="n6">6</a>
<a href="#n7" name="n7">7</a>
<a href="#n8" name="n8">8</a>
<a href="#n9" name="n9">9</a>
<strong><a href="#n10" name="n10">10</a></strong>
<a href="#n11" name="n11">11</a>
<a href="#n12" name="n12">12</a>
<a href="#n13" name="n13">13</a>
<a href="#n14" name="n14">14</a>
<a href="#n15" name="n15">15</a>
<a href="#n16" name="n16">16</a>
<a href="#n17" name="n17">17</a>
<a href="#n18" name="n18">18</a>
<a href="#n19" name="n19">19</a>
<strong><a href="#n20" name="n20">20</a></strong>
<a href="#n21" name="n21">21</a>
<a href="#n22" name="n22">22</a>
<a href="#n23" name="n23">23</a>
<a href="#n24" name="n24">24</a>
<a href="#n25" name="n25">25</a>
</pre></td>
  <td class="code"><pre>MPI_Status status;
MPI_Datatype MPI_darray;
<span style="color:#0a8;font-weight:bold">int</span> N = <span style="color:#00D">8</span>, nb = <span style="color:#00D">4</span>;
<span style="color:#0a8;font-weight:bold">int</span> dims[<span style="color:#00D">2</span>] = {N, N};
<span style="color:#0a8;font-weight:bold">int</span> distribs[<span style="color:#00D">2</span>] = {MPI_DISTRIBUTE_CYCLIC, MPI_DISTRIBUTE_CYCLIC};
<span style="color:#0a8;font-weight:bold">int</span> dargs[<span style="color:#00D">2</span>] = {nb, nb};
<span style="color:#0a8;font-weight:bold">int</span> proc_nrows = <span style="color:#00D">2</span>, proc_ncols = <span style="color:#00D">2</span>;
<span style="color:#0a8;font-weight:bold">int</span> proc_dims[<span style="color:#00D">2</span>] = {proc_nrows, proc_ncols};

MPI_Type_create_darray(
    num_procs, <span style="color:#777">// size of process group (positive integer)</span>
    proc_id, <span style="color:#777">// rank in process group (non-negative integer)</span>
    <span style="color:#00D">2</span>, <span style="color:#777">//         number of array dimensions as well as process grid dimensions (positive integer)</span>
    dims, <span style="color:#777">// number of elements of type oldtype in each dimension of global array (array of positive integers)</span>
    distribs, <span style="color:#777">// distribution of array in each dimension (array of state)</span>
    dargs, <span style="color:#777">// distribution argument in each dimension (array of positive integers)</span>
    proc_dims, <span style="color:#777">// size of process grid in each dimension (array of positive integers)</span>
    MPI_ORDER_C, <span style="color:#777">// array storage order flag (state)</span>
    MPI_INT, <span style="color:#777">// old datatype (handle)</span>
    &amp;MPI_darray <span style="color:#777">// new datatype (handle)</span>
);
MPI_Type_commit(&amp;MPI_darray);
MPI_Type_size(MPI_darray, &amp;darray_size);
nelements = darray_size / <span style="color:#00D">4</span>;
MPI_Type_get_extent(MPI_darray, &amp;lower_bound, &amp;darray_extent);
</pre></td>
</tr></table>
</div>

<p>For reading a file in MPI, you need to use the <code>MPI_File_*</code> functions. This involves opening the file like any other normal file, but that file is handled internally by MPI. You need to set a ‘view’ for the file for each MPI process, and then the process can ‘seek’ the appropriate location in the file and read the required data.</p>

<p>The following code in useful for this purpose:</p>
<div class="language-c highlighter-coderay"><table class="CodeRay"><tr>
  <td class="line-numbers"><pre><a href="#n1" name="n1">1</a>
</pre></td>
  <td class="code"><pre>
</pre></td>
</tr></table>
</div>

<p>Sometimes reading from files can give divide-by-zero errors,</p>

<p>Note on <code>MPI_File_set_view</code>: this function is used for setting a ‘file view’ for each process so that the process knows where to start the data reading from. In case you’re using <code>MPI_File_read_all</code> you should know that the file pointer is set implicitly and you don’t need to explicitly supply an offset value. The file pointer for the current view is set based on what the process previous to this process accessed.</p>

<p>A full program for performing a matrix multiplication using PBLAS and BLACS using a block cyclic data distribution can be found <a href="">here</a>. Some more docs are <a href="http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node73.htm">here</a>.</p>
