<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Travel &lt;code&gt; Music]]></title>
  <link href="http://v0dro.github.io/atom.xml" rel="self"/>
  <link href="http://v0dro.github.io/"/>
  <updated>2014-09-28T15:46:29+05:30</updated>
  <id>http://v0dro.github.io/</id>
  <author>
    <name><![CDATA[Sameer Deshmukh]]></name>
    <email><![CDATA[sameer.deshmukh93@icloud.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[code]Generalized Linear Models: Introduction and Implementation in Ruby.]]></title>
    <link href="http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby/"/>
    <updated>2014-09-21T19:05:21+05:30</updated>
    <id>http://v0dro.github.io/blog/2014/09/21/code-generalized-linear-models-introduction-and-implementation-in-ruby</id>
    <content type="html"><![CDATA[<h2 id="overview">Overview</h2>

<p>Most of us are well acquainted with linear regression and its use in analysig the relationship of one dataset with another. Linear regression basically shows the (possibly) linear relationship between one or more independent variables and a single dependent variable. But what if this relationship is not linear and the dependent and independent variables are associated with one another through some special function? This is where Generalized Linear Models (or GLMs) come in. This article will explain some core <a href="http://en.wikipedia.org/wiki/Generalized_linear_model">GLM</a> concepts and their implementation in Ruby using the <a href="https://github.com/sciruby/statsample-glm">statsample-glm</a> gem.</p>

<h2 id="generalized-linear-models-basics">Generalized Linear Models Basics</h2>

<p>The basic linear regression equation relating the dependent varible <em>y</em> with the independent variable <em>x</em> looks something like 
<script type="math/tex">
\begin{align}
    y = \beta_{0} + x*\beta_{1} 
\end{align}
</script>
This is the equation of a straight line, with <script type="math/tex"> \beta_{0} </script> denoting the intercept of the line with the Y axis and <script type="math/tex"> \beta_{1} </script> denoting the slope of the line. GLMs take this a step further. They try to establish a relationship between <em>x</em> and <em>y</em> through <em>another function</em> <strong>g(x)</strong>, which is called the <em>link function</em>. This function depends on the probability distribution displayed by the independent variables and their corresponding y values. In its simplest form, it can be denoted as <em>y = g(x)</em>.</p>

<p>GLMs exist in many forms and have different names depending on the distribution of the independent variables. We will first explore the various kinds of GLMs and their defining parameters and then understand the different methods employed in finding the co-efficients. The different kinds of GLMs are:</p>

<ul>
  <li>Logistic (or logit) regression.</li>
  <li>Normal regression.</li>
  <li>Poisson regression.</li>
  <li>Probit regression.</li>
</ul>

<p>Let’s see all of the above one by one.</p>

<h4 id="logisitic-regression">Logisitic Regression</h4>
<p>Logistic, or Logit can be said to be one of the most fundamental of the GLMs. It is mainly used in cases where the independent variables show binomial distribution. In case of binomial distribution, the corresponding <em>y</em> value for each random variable is either 0 or 1. By using logit link function, one can determine the maximum probability of the occurence of each independent random variable. The values so obtained can be used to plot a sigmoid graph of <em>x</em> vs <em>y</em>, using which one can predict the probability of occurence of any random varible not already in the dataset. The defining parameter of the logistic is the probability <em>y</em>.</p>

<p>The logit link function looks something like 
<script type="math/tex">
\begin{align}
    y = \frac{e^{(\beta_{0} + x*\beta_{1})}}{1 + e^{(\beta_{0} + x*\beta_{1})}}
\end{align}
</script>
, where y is the probability for the given value of x.</p>

<p>Of special interest is the meaning of the values of the coefficients. In case on linear regression, <script type="math/tex"> \beta_{0} </script> merely denotes the intercept while <script type="math/tex"> \beta_{1} </script> is the slope of the line. However, here, because of the nature of the link function, the coefficient <script type="math/tex"> \beta_{1} </script> of the independent variable is interpreted as “for every 1 increase in <em>x</em> the odds of <em>y</em> increase by <script type="math/tex"> e^{\beta_{1}} </script> times”.</p>

<p>One thing that puzzled me when I started off with regression was the purpose of having several variables <script type="math/tex"> (x_{1}, x_{2}...) </script> in the same regression model at times. The purpose of multiple independent variables against a single dependent is so that we can compare the odds of <script type="math/tex"> x_{1} </script> against <script type="math/tex"> x_{2} </script>.</p>

<p>The logistic graph generally looks like this:</p>

<p><img class="center" src="http://v0dro.github.io/images/glm/logistic.gif" title="&#34;Generic Graph of Logistic Regression.&#34;" alt="&#34;Generic Graph of Logistic Regression.&#34;" /></p>

<h4 id="normal-regression">Normal Regression</h4>

<p>Normal regression is used when the indepdent variables exihibit a normal probability distribution. The independents are assumed to be normal even in a simple linear or multiple regression, and the coefficients of a normal are more easily calculated using simple linear regression methods. But since this is another very important and commonly found data set, we will look into it.</p>

<p>Normally distributed data is symmetric about the center and its mean is equal to its median. Commonly found normal distributions are heights of people and errors in measurement. The defining parameters of a normal distribution are the mean <script type="math/tex"> \mu </script> and variance <script type="math/tex"> \sigma^2 </script>. The link function is simply <script type="math/tex"> y = x*\beta_{1} </script> if no constant is present. The coefficient of the independent variable is interpreted in exactly the same manner as it is for linear regression.</p>

<p>A normal regression graph generally looks like this:</p>

<p><img class="center" src="http://v0dro.github.io/images/glm/normal.png" title="&#34;Generic Graph of Normal Regression&#34;" alt="&#34;Generic Graph of Normal Regression&#34;" /></p>

<h4 id="poisson-regression">Poisson Regression</h4>

<p>A dataset often posseses a Poisson distribution when the data is measured by taking a very large number of trials, each with a small probability of success. For example, the number of earthquakes taking place in a region per year. It is mainly used in case of count data and contingency tables. Binomial distributions often converge into Poisson.</p>

<p>The poisson is completely defined by the rate parameter <script type="math/tex"> \lambda </script>. The link function is <script type="math/tex"> ln(y) = x*\beta_{1} </script>, which can be written as <script type="math/tex"> y = e^{x*\beta_{1}} </script>. Because the link function is logarithmic, it is also referred to as log-linear regression.</p>

<p>The meaning of the co-efficient in the case of poisson is “for increase 1 of <em>x</em>, <em>y</em> changes <script type="math/tex"> y = e^\beta_{1} </script> times.”. Notice that in logit, every 1 increase in the value of x caused the <em>odds</em> of y to change by <script type="math/tex"> y = e^\beta_{1} </script> times.</p>

<p>A poisson graph looks something like this:</p>

<p><img class="center" src="http://v0dro.github.io/images/glm/poisson.png" title="&#34;Graph of Poisson Regression&#34;" alt="&#34;Graph of Poisson Regression&#34;" /></p>

<h4 id="probit-regression">Probit Regression</h4>

<p>Probit is used for modeling binary outcome varialbles. Probit is similar to  logit, the choice between the two largely being a matter of personal preference.</p>

<p>In the probit model, the inverse standard normal distribution of the probability is modeled as a linear combination of the predictors (in simple terms, something like <script type="math/tex"> y = \Phi(\beta_{0} + x_{1}*\beta_{1}...) </script> , where <script type="math/tex"> \Phi </script> is the CDF of the standard normal). Therefore, the link function can be written as <script type="math/tex"> z = \Phi^{-1}(p) </script> where <script type="math/tex"> \Phi(z) </script> is the standard normal cumulative density function (here <em>p</em> is probability of the occurence of a random variable <em>x</em> and <em>z</em> is the z-score of the y value).</p>

<p>The fitted mean values of the probit are calculated by setting the upper limit of the normal CDF integral as <script type="math/tex"> x*\beta_{1} </script>, and lower limit as <script type="math/tex"> -\infty </script>. This is so because evaluating any normally distributed random number over its CDF will yield the probability of its occurence, which is what we expect from the fitted values of a probit.</p>

<p>The coefficient of <em>x</em> is interpreted as “one unit change in <em>x</em> leads to a change <script type="math/tex"> \beta_{1} </script> in the z-score of <em>y</em>”.</p>

<p>Looking at the graph of probit, one can see the similarities between logit and probit:</p>

<p><img class="center" src="http://v0dro.github.io/images/glm/probit.png" /></p>

<h2 id="finding-the-coefficients-of-a-glm">Finding the coefficients of a GLM</h2>

<p>There are two major methods of finding the coefficients of a GLM:</p>

<ul>
  <li>Maximum Likelihood Estimation (MLE).</li>
  <li>Iteratively Reweighed Least Squares (IRLS).</li>
</ul>

<h4 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h4>

<p>The most obvious way of finding the coefficients of the given regression analysis is by maximizing the likelihood function of the distribution that the independent variables belong to. This becomes much easier when we take the natural logarithm of the likelihood function. Hence, the name ‘Maximum Likelihood Estimation’. The Newton-Raphson method is used to this effect for maximizing the beta values (coefficients) of the log likelihood function.</p>

<p>The first derivative of the log likelihood wrt to <script type="math/tex"> \beta </script> is calculated for all the <script type="math/tex"> x_{i} </script> terms (this is the jacobian matrix), and so is the second derivative (this is the hessian matrix). The coefficient is estimated by first choosing an initial estimate for <script type="math/tex"> x_{old} </script>, and then iteratively correcting this initial estimate by trying to bring the equation</p>

<script type="math/tex; mode=display"> 
\begin{align}
x_{new} = x_{old} - inverse(hessian)*jacobian   ..(1) 
\end{align}
</script>

<p>to equality (with a pre-set tolerance level). A good implementation of MLE can be found <a href="http://petertessin.com/MaxLik.pdf">here</a>.</p>

<h4 id="iteratively-reweighed-least-squares">Iteratively Reweighed Least Squares</h4>

<p>Another useful but somewhat slower method of estimating the regression coefficients of a dataset is Iteratively Reweighed Least Squares. It is slower mainly because of the number of co-efficients involved and the somewhat extra memory that is taken up by the various matrices used by this method. The upside of IRLS is that it is very easy to implement as is easily extensible to any kind of GLM.</p>

<p>The IRLS method also ultimately boils to the equation of the Newton Raphson (1), but the key difference between the two lies in the manner in which the hessian and jacobian matrices are calculated. The IRLS equation is written as:</p>

<script type="math/tex; mode=display">
\begin{align}
    b_{new} = b_{old} - inverse(X'*W*X)*(X'*(y - \mu))
\end{align}
</script>

<p>Here, the hessian matrix is <script type="math/tex"> -(X'*W*X) </script> and the jacobian is <script type="math/tex"> (X'*(y - \mu)) </script>. Let’s see the significance of each term in each of these matrices:</p>

<ul>
  <li><em>X</em> - The matrix of independent variables  <script type="math/tex"> x_{1}, x_{2},... </script> alongwith the constant vector.</li>
  <li><em>X’</em> - Transpose of X.</li>
  <li><em>W</em> - The weight matrix. This is the most important entity in the equation and understanding it completely is paramount to gaining an understanding of the IRLS as whole.
    <ul>
      <li>The <em>weight</em> matrix is present to reduce favorism of the best fit curve towards larger values of x. Hence, the weight matrix acts as a mediator of sorts between the very small and very large values of x (if any). It is a diagonal matrix with each non-zero value representing the weight for each vector <script type="math/tex"> x_{i} </script> in the sample data.</li>
      <li>Calculation of the weight matrix is dependent on the probability distribution shown by the independent random variables. The weight expression can be calculated by taking a look at the equation of the hessian matrix. So in the case of logistic regression, the weight matrix is a diagonal matrix with the ith entry as <script type="math/tex"> p(x_{i}, \beta_{old})*(1 - p(x_{i}, \beta_{old})) </script>.</li>
    </ul>
  </li>
  <li><script type="math/tex"> (y - \mu) </script> - This is a matrix whose ith value the is difference between the actual corresponding value on the y-axis minus <script type="math/tex"> \mu = x*b_{old} </script>. The value of this term is crucial in determining the error with which the coefficients have been calculated. Frequently an error of 10e-4 is acceptable.</li>
</ul>

<h2 id="generalized-linear-models-in-ruby">Generalized Linear Models in Ruby</h2>

<p>Calculating the co-efficients and a host of other properties of a GLM is extremely simple and intuitive in Ruby. Let us see some examples of GLM by using the <code>statsample</code> and <code>statsample-glm</code> gems:</p>

<p>First install <code>statsample-glm</code> by running <code>gem install statsample-glm</code>, statsample will be downloaded alongwith it if it is not installed directly. Then download the CSV files from <a href="https://github.com/SciRuby/statsample-glm/blob/master/spec/data/logistic_mle.csv">here</a>.</p>

<p>Statsample-glm supports a variety of GLM methods, giving the choice of both, IRLS and MLE algorithms to the user for almost every distribution, and all this through a simple and intutive API. The primary calling function for all distribtions and algorithms is <code>Statsample::GLM.compute(data_set, dependent, method, options)</code>. We specify the data set, dependent variable, type of regression and finally an options hash in which one can specify a variety of customization options for the computation.</p>

<p>To compute the co-efficients of a logistic regression, try this code:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="ruby"><span class="line"><span class="c1"># Code for computing coefficients and related attributes of a logistic regression.</span>
</span><span class="line">
</span><span class="line"><span class="n">data_set</span> <span class="o">=</span> <span class="no">Statsample</span><span class="o">::</span><span class="no">CSV</span><span class="o">.</span><span class="n">read</span> <span class="s2">&quot;logistic_mle.csv&quot;</span>
</span><span class="line"><span class="n">glm</span> <span class="o">=</span> <span class="no">Statsample</span><span class="o">::</span><span class="no">GLM</span><span class="o">.</span><span class="n">compute</span> <span class="n">data_set</span><span class="p">,</span> <span class="ss">:y</span><span class="p">,</span> <span class="ss">:logistic</span><span class="p">,</span> <span class="p">{</span><span class="ss">constant</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="ss">algorithm</span><span class="p">:</span> <span class="ss">:mle</span><span class="p">}</span>
</span><span class="line"><span class="c1"># Options hash specifying addition of an extra constants vector all of whose values is &#39;1&#39; and also specifying that the MLE algorithm is to be used.</span>
</span><span class="line">
</span><span class="line"><span class="nb">puts</span> <span class="n">glm</span><span class="o">.</span><span class="n">coefficients</span>
</span><span class="line">  <span class="c1">#=&gt; [0.3270, 0.8147, -0.4031,-5.3658]</span>
</span><span class="line"><span class="nb">puts</span> <span class="n">glm</span><span class="o">.</span><span class="n">standard_error</span>
</span><span class="line">  <span class="c1">#=&gt; [0.4390, 0.4270, 0.3819,1.9045]</span>
</span><span class="line"><span class="nb">puts</span> <span class="n">glm</span><span class="o">.</span><span class="n">log_likelihood</span>
</span><span class="line">  <span class="c1">#=&gt; -38.8669</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Similar to the above code, you can try implementing poisson, normal or probit regression models and use the data files from the link above as sample data. Just go through the tests in the source code on GitHub or read the documentation for further details and feel free to drop me a mail in case you have any doubts/suggestions for improvements.</p>

<p>Cheers!</p>

<hr />

<h6 id="further-reading">Further Reading</h6>
<ul>
  <li><a href="https://cise.ufl.edu/class/cis6930sp10esl/downloads/LogisticRegression.pdf">A good explanation of IRLS</a>.</li>
  <li><a href="http://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf">Logistic Regression and Newtons Method</a>.</li>
  <li><a href="https://files.nyu.edu/mrg217/public/mle_introduction1.pdf">A good resource on the how and why behind the calculation of standard errors</a>.</li>
  <li><a href="http://www.columbia.edu/~so33/SusDev/Lecture_9.pdf">Logit and Probit</a>.</li>
  <li><a href="http://www.nesug.org/Proceedings/nesug10/sa/sa04.pdf">A very good explanation of the Poisson regression</a>.</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[code]Managing Large Open Source Projects: For Beginners.]]></title>
    <link href="http://v0dro.github.io/blog/2014/08/27/code-managing-large-open-source-projects-for-beginners/"/>
    <updated>2014-08-27T16:59:16+05:30</updated>
    <id>http://v0dro.github.io/blog/2014/08/27/code-managing-large-open-source-projects-for-beginners</id>
    <content type="html"><![CDATA[<p>I had just started writing some meaningful code in college and gaining an interest in scientific computation, and really wanted to apply my knowledge somewhere and create a real impact. That is when I read about the ruby <a href="https://github.com/sciruby/nmatrix">NMatrix</a> gem and thought maybe I’d contribute some code, both for experience in dealing with non-trivial software and the personal satifaction derived from the feeling of my work being used by people around the world.</p>

<p>I cloned and installed the source code like any other programmer would, and just as I first open the Issue tracker to find a project to work on, I was hit by an avalanche of information ranging from bugs, documentation appeals, and new feature requests. And this was nothing compared to my first reaction upon going through the source code. Having never worked with more than a few files of source in college, I was dwarfed by the scores of source files and hundreds of method definitions that hung above me like a mountain. I quickly discovered that software is no small thing and that to make real, non-trivial, production grade software requires a certain discipline, focus and patience.</p>

<p>I had never dealt with something like this before, and set out to discover books/courses that would enlighten me on the topic. Most of the blog posts that I came across told me to keep looking through the code and that eventually I would become good at it. While this is true, none spoke of any specific methods to use to get out of this dilemma. It was then that I came across the course <a href="https://www.coursera.org/course/learning">Learning How To Learn</a>, and after enrolling and seeing it through, I must say I have gained quite an insight into managing large software and even my day to day activities, the course having encompassed a large variety of practtical scenarios.</p>

<p>I have written this blog post for the final assignment of the course, which asks me to share the insights I have gained with people who might be in the same dilemma that I was in. I hope you find it helpful.</p>

<p>When first faced with something that you are not trained to handle, you tend to get overwhelhmed and try to avoid it, even dislike it. But this takes you nowhere close to the goal that you might want to accomplish. You just stagnate in the exact same spot that you initially were in and you never really move ahead.</p>

<p>To move ahead, it is important to focus your energies and force yourself to work on a particular problem for a length of time. But how does one do this? And for what length of time? After taking the course, I learned that the mind, at any time, is basically in two modes of thinking, the focused mode and the diffused mode.</p>

<p>Your mind is in the focused mode when you are intently focusing on, say implementing a tree travelsal algorithm that your professor might have told you to implement. Focused mode is required in situations where you know <em>exactly</em> what you’re looking for and want to devise a method of getting there.</p>

<p>The focused mode, however, is not much use when first faced with a large software project. Focusing on just one aspect of the system the first time will leave you more confused than before and more often than not, any changes that you make will create more problems that you’d want to care about.</p>

<p>Scenarios like these are where the diffused mode comes to the rescue. In the diffused mode, the mind is capable of thinking about many things at a time, maybe not making sense of them all, but creating connections between them nonetheless. It is a phase of light concentration that your mind goes through, with the problem you want to solve lightly running in the background. The diffused mode is what helps in making sense of a large project.</p>

<p>You must first learn to relax, sit back and take stock of the entire project and at the same time keep in mind the new functionality that you want to implement or the bug that you want to quash. Try to simply read the names of the files and folders and try to connect them with your problem. Most Open Source projects use very strict conventions and upon lightly thinking for a while you will stumble upon a particular file or folder that will be relevant to what you are looking for.</p>

<p>The diffused mode will only help you in getting a larger picture of things. Once you have a tentative idea of where you might find the problem area, then it is time to switch to the focused mode and dive into ONLY that particular file/function that you think is the right one. Do not think of anything else while searching in the place your diffused mode has taken you to.</p>

<p>You first think in the diffused mode, and then the focused mode. Keep repeating this procedure until you solve the problem, and you will find that eventually, you can intuitively figure out where a particular line of code might reside.</p>

<p>Thinking in the focused mode requires practice, and you will soon realize that you tend to feel distracted after some focused thinking. This is where certain techniques for focused mode thinking come into play. One of the best and easiest to practice is the ‘Pomodoro’ technique, which advocates being focused for 25 min. and then taking a short break for 5 min., then keep repeating this cycle until you think you’ve had enough. </p>

<p>While focusing it is extremely important to focus ONLY on the problem at hand and nowhere else. You should typically sit in a quiet environment and away from distractions if you want pomodoro to work for you. While taking a break, do some light activity, like taking a walk around your work area or watching a small TED talk. Pomodoro has worked wonders for me and I highly recommend using it. You can use one of the scores of mobile apps available for setting a pomodoro timer.</p>

<p>One of the most dangerous things to be absolutely careful about is procrastination. It is very easy to get carried away by some fancy code that you come across for the first time while tracing method calls  and completely forget about the problem that you are trying to tackle. Procrastination happens when you allow yourself to get carried away. It leads you to think that you’ve done a lot of work, when in reality you have done nothing.</p>

<p>If faced by a somewhat difficult problem, write it down on a Post-It note and stick it in a place that is always within your field of view, like in my case, my desk or the palm rest of my laptop. Keep looking at what’s written on this note and periodically ask yourself, “Am I closer to solving the problem than I was before?”, “Will the particular line of code that I am reading right now be of any use in solving this problem?”. If your answers to these questions are negative, you need to realign yourself and remind yourself to get back to work.</p>

<p>Over and above the techniques mentioned above, also remember to break your problem down into small, manageable chunks, and go after one chunk at a time. Also, be sure to mentally go over these chunks once you’re done with your current session so that things will be more clear next time.If you’re having problems in visualizing an algorithm or the flow of a program, take a piece of paper and write down whatever you understand, and you will find that the rest will become clear once you ponder over what you have written. Theres only so much that your memory can store.</p>

<p>Over and above, have fun programming. Its a great thing to do, really.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Travel] Hampi-Bengaluru-Allepy Part 1]]></title>
    <link href="http://v0dro.github.io/blog/2014/07/30/travel-hampi-bengaluru-allepy-part-1/"/>
    <updated>2014-07-30T23:31:46+05:30</updated>
    <id>http://v0dro.github.io/blog/2014/07/30/travel-hampi-bengaluru-allepy-part-1</id>
    <content type="html"><![CDATA[<h2 id="hampi---day-1">Hampi - Day 1</h2>

<p>Our Engineering exams were just over, and we decided to get out of the rut by going backpacking, and chose South India as an ideal destination. Since we had only one week to spare due to family commitments, we chose a route that would take us down south and give us a dose of both heritage and leisure.</p>

<p>We refered to some posts on the internet, and concluded that Hampi would be an ideal starting point. The erstwhile capital of the mighty Vijaynagar empire, Hampi saw a period of power and growth under the tutelage of King Krishnadevaraya, who was a fantastic strategist, administrator and a patron of the arts.</p>

<p>Althogh destroyed due to invasions of the Deccan Sultanate, the remains of Hampi still bear testament to the craftsmanship of Indian artists during the middle ages and the foresight and economic might of their patrons.</p>

<p>We took one of the regular buses that ply between Pune and Hospet, and reached Hospet at 8 am, after which a ride in a rickshaw took us to Hampi. The first thing  one notices while riding into Hampi is the massive, ornately decorated <a href="http://en.wikipedia.org/wiki/Gopuram">gopuram</a> of the <a href="http://en.wikipedia.org/wiki/Virupaksha_Temple">Virupaksha Temple</a>. This is also one of the few temples in Hampi that is still actively used in worship.</p>

<p><img src="http://v0dro.github.io/images/hampi-bengaluru-allepy/virupaksha.JPG" title="'The Virupaksha Temple'" /></p>

<p>We checked into a hotel which was a 5 minute walk from the Virupaksha temple, which charged us Rs. 800 per night. Since Hampi is a favorite for foreign tourists, one can find many cafes and restaurants here which cater to their needs. We had a  pot of tea at the German Bakery in the market area in morning, and planned for the day’s sightseeing.</p>

<p>We started out toward the Vitthal temple, taking the huge road that was once the Hampi Bazaar. Along the way, we saw the monolithic Nandi statue on the left, and <a href="http://hampi.in/matunga-hill">Matanga hill</a> on the right.</p>

<p><img class="left" src="http://v0dro.github.io/images/hampi-bengaluru-allepy/nandi.JPG" width="365" height="300" title="'The Nandi Statue'" /> <img class="right" src="http://v0dro.github.io/images/hampi-bengaluru-allepy/entrance.JPG" width="365" height="300" title="'Entrance At The Far End Of Hapmi Bazaar.'" /></p>

<p>A flight of steps lead to a grand entrance which opens at the Achyutraya Temple. This temple was build by the successor of King Krishnadevaraya, Achyuta Deva Raya, and is a temple to Lord Venkateshwara, but its ruins are popularly reffered to by the name of its patron. </p>

<p><img src="http://v0dro.github.io/images/hampi-bengaluru-allepy/achyutraya_temple.JPG" title="'Achyutraya Temple'" /></p>

<p>After walking over some huge rocks by the river, we reached the King’s Balance, a stone weighing scale where the King would be weighed against gold, which would then be given away to the priests. We then (finally) reached the Vitthal Temple.</p>

<p><img src="http://v0dro.github.io/images/hampi-bengaluru-allepy/vitthal_front.JPG" title="'The Sun Chariot With The Vitthal Temple In The Background'" /></p>

<p>The Vithhal Temple is probably one of the grandest in Hampi and possibly in all of India. It is housed inside a massive complex with ornately carved walls, entrances and pillars. The complex houses five mantapas, 4 in corners and 1 in the centre. The Sun Chariot from the <a href="http://en.wikipedia.org/wiki/Konark_Sun_Temple">Konark temple</a> has been replicated here. The vehicle of Vitthal, Garuda (eagle) can be seen inside the chariot. The central mantap houses the famous singing pillars. These stone pillars have been designed in such a way as to create sounds of different musical instruments when hit by a stick or any  other object. The Queen would dance for the King in this mantap to the sound of music from the pillars. The roof of the central mantap was blown up by invaders in 1565 A.D., so the sound is fairly diminished now.</p>

<p>The central mantap houses an inner temple, where the actual idols were kept and worshipped. The original idols of Vitthal and Rukumai were taken to Pandharpur in Maharashtra during the invasion of Vijaynagar. One unique feature  of the Vitthal temple is that the place where one performs <a href="http://en.wikipedia.org/wiki/Parikrama">‘pradakshina’</a> is underground.</p>

<p><img class="right" src="http://v0dro.github.io/images/hampi-bengaluru-allepy/underground_parikrama.JPG" width="300" height="500" title="'The Underground Pradakshina Path'" /></p>

<p>There are tiny inlets for light in the roof, which reflects off a stream of water on the floor, which in turn provides illumination for the entire chamber.</p>

<p>We then proceeded towards the river, where the local people offer to take you downstream in a <a href="http://en.wikipedia.org/wiki/Coracle">Coracle</a> (locallly reffered to as ‘joutty’) boat. We were sitting in this kind of a boat for the first time and this turned out to be one hell of a boat ride, with our guide taking the boat under overhanging rocks and letting it spin wildly every now and then. The river is also lined with ruins of temples and everything is quite a pleasure to watch. We disembarked at the Varana temple, which is a big white temple and is still functional. Terribly hungry, we walked to the main temple and ate delicious cheese tomato omelletes at Cafe Chillout.</p>

<p>We rested for a while and then decided to pay a visit to the Virupaksha temple. This is a <em>massive</em> temple. It is mostly functional and the inner complex is mostly intact. One interesting facet of this temple is that one can see the inverse image of the main gopuram in a pond behind the Shivling.</p>

<p>We then proceeded to our hotel for a good night’s sleep.</p>

]]></content>
  </entry>
  
</feed>
