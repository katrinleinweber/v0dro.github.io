---
layout: post
title: "Thoughts on writing your own compiler."
date: 2016-11-24 21:40:55 +0530
comments: true
categories: 
---

I'm writing Rubex. It uses racc, an LALR(1) parser generator. Need to write parsers according to the LALR(1) grammar. Started with a [simple description](http://web.cs.dal.ca/~sjackson/lalr1.html) of LALR(1) grammars. Found a grammar checker [here](http://smlweb.cpsc.ucalgary.ca/start.html).

In an LALR(1) grammar, left recursion is often preferred and seems safer.

Parser is just one stage of this, the next challenge is building an AST that can effectively represent your language in such a way that you can easily represent any syntactical and semantic data with the parsed constructs and the different parts of the compiler can communicate effectively with each other.

According to Language Implementation Patterns page 92, ASTs should have the following behaviour:
- Dense: No unnecessary nodes
- Convenient: Easy to walk
- Meaningful: Emphasize operators, operands, and the relationship between them rather than artifacts from the grammar.

Something important to remember when building an AST is that it must be decoupled from the grammar of the language, since grammars change all the time, and this shoul not impact other components of the language. The key idea behind AST structure is that tokens representing operators or operations become subtree roots. All other tokens become operands (children of operator nodes).

Type structure or syntax structure can be enforced by the AST. Whenever a node is being populated, one can know if it is of the right type and raise an error if it is not.

Type declarations can also be recorded in the AST, but we need to create a special 'imaginary token' to represent these. So for example, a token like `VARDECL` can be used for declaring C-like variables that look like `int i;`. In this case the children of the `VARDECL` node will be `int` and `i`.

There are basically three steps in processing the AST:
  #  There are 3 phases of parse tree processing, applied in order to
  #  all the statements in a given scope-block:
  #
  #  (1) analyse_declarations
  #        Make symbol table entries for all declarations at the current
  #        level, both explicit (def, cdef, etc.) and implicit (assignment
  #        to an otherwise undeclared name).
  #
  #   (2) analyse_expressions
  #         Determine the result types of expressions and fill in the
  #         'type' attribute of each ExprNode. Insert coercion nodes into the
  #         tree where needed to convert to and from Python objects. 
  #         Allocate temporary locals for intermediate results. Fill
  #         in the 'result' attribute of each ExprNode with a C code
  #         fragment.
  #
  #   (3) generate_code
  #         Emit C code for all declarations, statements and expressions.
  #         Recursively applies the 3 processing phases to the bodies of
  #         functions.

I am reading the Pyrex source as a guide of sorts. In this, he has written his own parser. The code is generated by calling the Nodes.ModuleNode.process() method.

Now we also need a symbol table for knowing where each statement exists and what it signifies. This symbol table interacts with the AST and generates entries in the table based on information in the AST. While doing this, the AST is also populated with entries that make it more useful. I am having a look at [this article](http://what-when-how.com/compiler-writing/symbol-table-handling-techniques-compiler-writing-part-1/) to understand symbol tables better. The [wiki](https://en.wikipedia.org/wiki/Symbol_table) is also nice.

every occurrence of an identifier in a source program requires some symbol-table interactio.

A compiler may use one large symbol table for all symbols or use separated, hierarchical symbol tables for different scopes. 

In hierarchical symbol tables, the symbol table entries for Methods have two parts - one which holds the method arguments (the method symbol itself) and the other which holds the locally declared variables of the method.

Now one might think that why is storing information in scopes so necessary if we can store stuff in the nodes of the AST anyway. The reason for this is that, it so happens that the scope needs to be passed around to a lot of objects when the AST is being populated with more information. A lot of these objects might belong somewhere down the hierarchy of the AST and it is important for these objects to have access to the symbols that they operating in to acess more information about them and enhance themselves. Hence, storing information in the hierarchical symbol table is important.

One of the principal requirements of any compiler is type checking and type inference. This is a necessary feature especially if your language is statically typed and knowing the result data type of an expression is necessary. I'm referring to chapter 6.5 (Type Checking) of the Dragon book for this purpose.

A type hierarchy defines which types are wider than which other types. Types lower in the hierarchy can be casted to types higher up in the hierarchy. Other way round is usually not done since it leads to loss of information.

The semantic action for checking an expression E -> E1 + E2 uses two functions mainly:
- max(t1, t2) - Takes two types t1 and t2 and returns the maximum of the two types in the widening hierarchy. Declares error if either types are not in hierarchy.
- widen(a,t,w) - generates type conversions if needed to widen the contents of an address a of type t into a value of type w . It returns a itself if t and w are the same type.

One tricky thing is the null rule in the LALR(1) parser. The thing with this is that it basically tells you that if nothing on the stack matches with a rule, and a null rule is present, 'nothing' will be reduced to that rule and the respective production will make its way on top of the stack. This rule must be remembered at all times when writing grammars since it can wreak havoc. See this SO answer: http://stackoverflow.com/questions/8242509/how-does-the-yacc-bison-lalr1-algorithm-treat-empty-rules

Another important concept is associativity of operators. The reason why it is important is covered nicely over here: http://stackoverflow.com/questions/930486/what-is-associativity-of-operators-and-why-is-it-important


Take these rules for example:
bodystmt: stmts opt_terms
stmts: {} | stmt | stmts terms stmt

When an else block is written as 'else: kELSE stmts' it will fail because the 'stmts terms' will first get matched and the parser will try to find a 'stmt' and it will fail. Replacing that with 'bodystmt' will solve the issue since now the parser can get away with matching 'stmt opt_terms' (stmts resolves to stmt).